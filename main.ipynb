{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "infrared-newfoundland",
   "metadata": {},
   "source": [
    "## Investigating the Zillow Housing Price Dataset\n",
    "\n",
    "N. Ranjan & R. Mattson | CSCI 6430* | Mar 11, 2021\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "TODO: \n",
    "- check file names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mental-calculator",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "from sklearn.metrics import r2_score as r2score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "#from sklearn.metrics import mean_absolute_percentage_error as mape\n",
    "#from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.linear_model import Lasso,Ridge\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecological-lightweight",
   "metadata": {},
   "source": [
    "###  Retrieve Data and Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "impressive-organ",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO - get from google docs??\n",
    "DF1B = pd.read_csv(r'Datasets/City_MedianRentalPrice_1Bedroom.csv')\n",
    "DF2B = pd.read_csv(r'Datasets/City_MedianRentalPrice_2Bedroom.csv')\n",
    "DF3B = pd.read_csv(r'Datasets/City_MedianRentalPrice_3Bedroom.csv')\n",
    "DF4B = pd.read_csv(r'Datasets/City_MedianRentalPrice_4Bedroom.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "impossible-kentucky",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "thick-merchandise",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove columns with useless information - These were only indexes\n",
    "DF1B.drop(columns=['SizeRank','Unnamed: 0'],inplace=True)\n",
    "DF2B.drop(columns=['SizeRank','Unnamed: 0'],inplace=True)\n",
    "DF3B.drop(columns=['SizeRank','Unnamed: 0'],inplace=True)\n",
    "DF4B.drop(columns=['SizeRank','Unnamed: 0'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "everyday-biodiversity",
   "metadata": {},
   "outputs": [],
   "source": [
    "# record the number of bedrooms within the frame\n",
    "DF1B.insert(0, 'BedroomsSq', 1)\n",
    "DF2B.insert(0, 'BedroomsSq', 2)\n",
    "DF3B.insert(0, 'BedroomsSq', 3)\n",
    "DF4B.insert(0, 'BedroomsSq', 4)\n",
    "#BE CAREFUL WITH INSERTING BEDROOMS\n",
    "insertBedrooms = np.where(DF1B.columns=='2010-02')[0][0] \n",
    "print(int(insertBedrooms))\n",
    "DF1B.insert(int(insertBedrooms), 'Bedrooms', 1)\n",
    "DF2B.insert(int(insertBedrooms), 'Bedrooms', 2)\n",
    "DF3B.insert(int(insertBedrooms), 'Bedrooms', 3)\n",
    "DF4B.insert(int(insertBedrooms), 'Bedrooms', 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "environmental-opportunity",
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = [DF1B,DF2B,DF3B,DF4B]\n",
    "result = pd.concat(frames)\n",
    "result = result.sample(frac=1) #Shuffle!\n",
    "result = result.reset_index(drop=True) #Reset Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceramic-communication",
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "desirable-message",
   "metadata": {},
   "outputs": [],
   "source": [
    "# result.to_csv(r'Datasets\\City_MedianRentalPrice_AllHomes.csv')\n",
    "result.to_csv(r'Datasets/City_MedianRentalPrice_AllHomes_ALTERED.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "official-abraham",
   "metadata": {},
   "outputs": [],
   "source": [
    "del DF1B,DF2B,DF3B,DF4B\n",
    "del result,frames\n",
    "#Just saving memory, let's continue with our final dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "square-minimum",
   "metadata": {},
   "source": [
    "### Preproccess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "composed-circle",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DF = pd.read_csv(r'Datasets\\City_MedianRentalPrice_AllHomes.csv')\n",
    "DF = pd.read_csv(r'Datasets/City_MedianRentalPrice_AllHomes_ALTERED.csv',index_col=0)\n",
    "# TrainDF = DF.iloc[:,:-1].copy() # Rest of the columns come here as training data\n",
    "# TestDF = DF.iloc[:,-1].copy()   # We predicting the last column\n",
    "# del DF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lb=LabelEncoder()\n",
    "lb.fit(DF['State'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "RecordAnalysisDF = pd.DataFrame(columns=['RunNumber','Title','Algorithm','R2Mean','R2Median','RMSEMean','RMSEMedian']) #Will save results for later\n",
    "runNo = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "isBedStateOptimized = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cut_off = 0.25 # if number of nulls > 30%, just remove the columns\n",
    "row_count = DF.index.size\n",
    "NullCountDF = pd.DataFrame({'Null_Count':DF.isnull().sum().to_numpy(),'Col_Name':DF.columns.to_numpy()})\n",
    "colsToKeep = NullCountDF[NullCountDF.Null_Count<=row_count*cut_off].Col_Name.to_numpy() #columns to keep,rest have too many nulls\n",
    "DF = DF[colsToKeep].copy() \n",
    "print(DF.isnull().sum().to_numpy())\n",
    "print(colsToKeep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing Nulls\n",
    "def RemoveNulls(TheDF):\n",
    "   print('Removing Nulls...')\n",
    "   for col in TheDF.columns:\n",
    "      for x in TheDF[TheDF[col].isnull()].index: #Going through all columns\n",
    "         beds = TheDF.loc[x]['Bedrooms']\n",
    "         state = TheDF.loc[x]['State']\n",
    "         #print)\n",
    "         if(isBedStateOptimized == True):\n",
    "            valFill = TheDF[(TheDF['Bedrooms'] == beds) & (TheDF['State']==state)][col] #Bedroom and state\n",
    "            if np.all(np.isnan(valFill)):\n",
    "               valFill = TheDF[(TheDF['Bedrooms'] == beds)][col] #Check if we can do only with bedroom as this is more correlated than state\n",
    "            if np.all(np.isnan(valFill)):\n",
    "               valFill = TheDF[(TheDF['State'] == beds)][col] #If only bedrooms didnt work, try with State\n",
    "            if np.all(np.isnan(valFill)):\n",
    "               valFill =TheDF[col] #If we get null for those too..just take median of whole column\n",
    "            valFill = np.nanmedian(valFill)\n",
    "            TheDF.loc[x,col] = valFill\n",
    "         else:\n",
    "            valFill =TheDF[col]\n",
    "            valFill = np.nanmedian(valFill)\n",
    "            TheDF.loc[x,col] = valFill\n",
    "   print('Nulls Removed.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RemoveNulls(DF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Assumed Last Column is Target, it should return Xtrain,Ytrain,Xtest,Ytest\n",
    "def ChangeDataSetForTimeSeries(TheDF):\n",
    "    posOfFirstDate = np.where(TheDF.columns=='Bedrooms')[0][0] + 1\n",
    "    FirstDate = TheDF.columns[posOfFirstDate]\n",
    "    T = 10\n",
    "    Startindex = np.where(TheDF.columns==FirstDate)[0][0]\n",
    "    #print(TheDF.columns[Startindex])\n",
    "    TrainEndindex = TheDF.columns.size-3\n",
    "    #ValidationEndindex = TheDF.columns.size-3\n",
    "    TestEndindex = TheDF.columns.size-2\n",
    "    #print(TheDF.columns[TrainEndindex])\n",
    "    size = TrainEndindex - Startindex + 1\n",
    "    colListTrain = TheDF.columns[0:Startindex].tolist()\n",
    "    for i in range(T):\n",
    "        colListTrain.append('T'+str(i))\n",
    "    TrainDFX = pd.DataFrame(columns=colListTrain[:-1])\n",
    "    TrainDFY = pd.DataFrame(columns=['T'+str(T)])\n",
    "    #print(colListTrain,len(colListTrain))\n",
    "    #Make Train-------------------------------\n",
    "    X_Arr = []\n",
    "    Y_Arr = []\n",
    "    for i in range(len(TheDF.index)): #Row Iteration\n",
    "        if(i%25==0):\n",
    "            print('\\r', 'Iteration', i+1, ' / Rows:', len(TheDF.index), end='')\n",
    "        initdataToInsert = TheDF.iloc[i,0:Startindex].to_numpy().tolist()\n",
    "        #print('-')\n",
    "        #print(initdataToInsert)\n",
    "        #print('-')\n",
    "        for t in range(Startindex,TrainEndindex + 1 - T):\n",
    "            x = TheDF.iloc[i,t:t+T].to_numpy().tolist()\n",
    "            #print(x)\n",
    "            #x = np.concatenate((initdataToInsert,x))\n",
    "            X_Arr.extend(initdataToInsert)\n",
    "            X_Arr.extend(x)\n",
    "            y = TheDF.iloc[i,t+T]\n",
    "            Y_Arr.append(y)\n",
    "            #return X_Arr,Y_Arr\n",
    "        # X_Arr = np.array(X_Arr).reshape(-1,len(colListTrain))\n",
    "        # Y_Arr = np.array(Y_Arr).reshape(-1,1)\n",
    "        \n",
    "        # X_Arr = pd.DataFrame(X_Arr,columns=colListTrain).to_dict()\n",
    "        # Y_Arr = pd.DataFrame(Y_Arr,columns=['T10'])\n",
    "        # #print(X_Arr)\n",
    "        # TrainDFX = TrainDFX.append(X_Arr,ignore_index=True)\n",
    "        # TrainDFY = TrainDFY.append(Y_Arr,ignore_index=True)\n",
    "        #break\n",
    "        #return TrainDFX,TrainDFY\n",
    "        #for ind in ():\n",
    "            #print(X)\n",
    "            #FinalDFX.loc[ind] = X[:,ind]\n",
    "            #FinalDFY.loc[ind] = Y.reshape(-1,1)\n",
    "            #break\n",
    "        #break   \n",
    "    print('\\r', 'Iteration', i+1, ' / Rows:', len(TheDF.index), end='') \n",
    "    X_Arr = np.array(X_Arr).reshape(-1,len(colListTrain))\n",
    "    Y_Arr = np.array(Y_Arr).reshape(-1,1)\n",
    "    \n",
    "    X_Arr = pd.DataFrame(X_Arr,columns=colListTrain)\n",
    "    Y_Arr = pd.DataFrame(Y_Arr,columns=['T10'])\n",
    "    #print(X_Arr)\n",
    "    TrainDFX = TrainDFX.append(X_Arr,ignore_index=True)\n",
    "    TrainDFY = TrainDFY.append(Y_Arr,ignore_index=True)\n",
    "    #Training Sets Done!!!!!\n",
    "    testColNames = TheDF.columns[0:Startindex].to_numpy().tolist()\n",
    "    #valColData = TheDF.columns[0:Startindex].to_numpy().tolist()\n",
    "    testColData = TheDF.columns[0:Startindex].to_numpy().tolist()\n",
    "    for i in range(T):\n",
    "        testColNames.append('T'+str(i))\n",
    "        #valColData.extend([TheDF.columns[ValidationEndindex-T+i+1]])\n",
    "        testColData.extend([TheDF.columns[TestEndindex-T+i+1]])\n",
    "    #Get Data of Validation and Testing.\n",
    "    print('----')\n",
    "    #print(valColData,testColData)\n",
    "    #ValDFX = TheDF[valColData].copy()\n",
    "    #ValDFX.columns = valAndTestColNames\n",
    "    #ValDFY = TheDF.iloc[:,ValidationEndindex+1].copy()\n",
    "    #ValDFY.columns = 'T'+str(T)\n",
    "    # ValidationEndindex\n",
    "    TestDFX = TheDF[testColData].copy()\n",
    "    TestDFX.columns = testColNames\n",
    "    TestDFY = TheDF.iloc[:,TestEndindex+1].copy()\n",
    "    TestDFY.columns = 'T'+str(T)\n",
    "    #return TrainDFX,TrainDFY,ValDFX,ValDFY,TestDFX,TestDFY\n",
    "    return TrainDFX,TrainDFY,TestDFX,TestDFY\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "TrainDFX,TrainDFY,TestDFX,TestDFY = ChangeDataSetForTimeSeries(DF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TrainDFX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lb = LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Found out Bedroom was more correlated than these and then on 2nd place is State\n",
    "# TrainDF['RegionName'] = lb.fit_transform(TrainDF['RegionName'])\n",
    "# TrainDF['State'] = lb.fit_transform(TrainDF['State'])\n",
    "# TrainDF['Metro'] = lb.fit_transform(TrainDF['Metro'])\n",
    "# TrainDF['CountyName'] = lb.fit_transform(TrainDF['CountyName'])\n",
    "#TrainDF['State'] = lb.fit_transform(TrainDF['State'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TrainDF.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "balanced-greensboro",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Pass by Value Reference Magic!\n",
    "def AddStateSparseMatrix(TrainingDF):\n",
    "   enc = OneHotEncoder(handle_unknown='ignore',sparse = False)\n",
    "   SM = enc.fit_transform(TrainingDF['State'].to_numpy().reshape(-1,1))\n",
    "   for ind,cat in zip(range(0,len(enc.categories_[0])),enc.categories_[0]):\n",
    "      TrainingDF.insert(0,'is'+str(cat),SM[:,ind])\n",
    "   TrainingDF.drop(columns=['State'],inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reserved-shopper",
   "metadata": {},
   "source": [
    "### Run Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RunAllModels(TrainDF,Output,title):\n",
    "    global runNo\n",
    "    global RecordAnalysisDF\n",
    "    global isBedStateOptimized\n",
    "    def PrintMetrics(scores,algo):\n",
    "        global RecordAnalysisDF\n",
    "        print('-'*5)\n",
    "        meanR2 = np.absolute(scores['test_r2']).mean()\n",
    "        medianR2 = np.median(np.absolute(scores['test_r2']))\n",
    "        meanRMSE = np.absolute(scores['test_neg_root_mean_squared_error']).mean()\n",
    "        medianRMSE = np.median(np.absolute(scores['test_neg_root_mean_squared_error']))\n",
    "        meanMAPE = np.absolute(scores['test_neg_mean_absolute_percentage_error']).mean()\n",
    "        medianMAPE = np.median(np.absolute(scores['test_neg_mean_absolute_percentage_error']))\n",
    "        print('Average R2 score ', meanR2)\n",
    "        print('Median R2 score ', medianR2)\n",
    "        print('Average Root Mean Square Error ', meanRMSE)\n",
    "        print('Median Root Mean Square Error ', medianRMSE)\n",
    "        print('-'*5)\n",
    "        if(isBedStateOptimized==False):\n",
    "            algo = algo + ' Nulls removed with complete column'\n",
    "        RecordAnalysisDF = RecordAnalysisDF.append({'RunNumber':runNo,'Title':title,'Algorithm':algo,'R2Mean':meanR2,'R2Median':medianR2,'RMSEMean':meanRMSE,'RMSEMedian':medianRMSE},ignore_index=True)\n",
    "        #print('Average Mean Absolute Percentage Error ', meanMAPE)\n",
    "        #print('Median Mean Absolute Percentage Error ', medianMAPE)\n",
    "    #-----------------------Linear Regression---------------------------------------------\n",
    "    print(title)\n",
    "    print('-'*10)\n",
    "    print('Linear Regression')\n",
    "    model = make_pipeline(StandardScaler(),LinearRegression())\n",
    "    # Y_pred = model.predict(X_test)\n",
    "    # Y_pred = scY.inverse_transform(Y_pred)\n",
    "    # Y_test = scY.inverse_transform(Y_test)\n",
    "    scores = cross_validate(model, TrainDF, Output, cv=15,scoring=('r2', 'neg_root_mean_squared_error','neg_mean_absolute_percentage_error'))\n",
    "    PrintMetrics(scores,'Linear Regression')#,meanMAPE,medianMAPE)\n",
    "    #----------------------------Polynomial Lasso---------------------------------------\n",
    "    for _,degree in enumerate([1]):\n",
    "        polymodel = make_pipeline(StandardScaler(),PolynomialFeatures(degree),Lasso(alpha=1e-2,max_iter=1e+6,tol=1e-2))\n",
    "        scores = cross_validate(polymodel, TrainDF, Output, cv=15,scoring=('r2', 'neg_root_mean_squared_error','neg_mean_absolute_percentage_error'))\n",
    "        print('Polynomial Degree with Lasso: '+str(degree))\n",
    "        PrintMetrics(scores,'Lasso Degree ' + str(degree))\n",
    "\n",
    "        polymodel = make_pipeline(StandardScaler(),PolynomialFeatures(degree),Ridge(alpha=1e-3,max_iter=1e+6))\n",
    "        scores = cross_validate(polymodel, TrainDF, Output, cv=15,scoring=('r2', 'neg_root_mean_squared_error','neg_mean_absolute_percentage_error'))\n",
    "        print('Polynomial Degree with Ridge: '+str(degree))\n",
    "        PrintMetrics(scores,'Ridge Degree ' + str(degree))\n",
    "    #----------------------------DecisionTreeRegressor---------------------------------------\n",
    "    model = make_pipeline(StandardScaler(),DecisionTreeRegressor(max_depth=100))\n",
    "    scores = cross_validate(model, TrainDF, Output, cv=15,scoring=('r2', 'neg_root_mean_squared_error','neg_mean_absolute_percentage_error'))\n",
    "    print('DecisionTreeRegressor')\n",
    "    PrintMetrics(scores,'DecisionTreeRegressor')\n",
    "    runNo = runNo + 1\n",
    "    print('-'*5)\n",
    "    print('-'*15)"
   ]
  },
  {
   "source": [
    "### Method 1: Drop all the Categorical columns"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "FinalTrainDF = TrainDFX.drop(columns=['RegionName', 'Metro', 'CountyName'])\n",
    "# RemoveNulls(FinalTrainDF,True)\n",
    "FinalTrainDF.drop(columns=['State'],inplace=True)\n",
    "RunAllModels(FinalTrainDF,TrainDFY,'Dropped Categorical Data')"
   ]
  },
  {
   "source": [
    "### Method2: Next, Let's change States to a sparse matrix and then to 51 columns, 1 for each State. Lets drop the other categorical columns"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "FinalTrainDF = TrainDFX.drop(columns=['RegionName', 'Metro', 'CountyName'])\n",
    "#RemoveNulls(FinalTrainDF,True)\n",
    "AddStateSparseMatrix(FinalTrainDF)\n",
    "#FinalTrainDF.drop(columns=['State'],inplace=True)\n",
    "RunAllModels(FinalTrainDF,TrainDFY,'One Hot Encoded State,Removed Others')\n",
    "#FinalTrainDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RecordAnalysisDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RecordAnalysisDF.index +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RecordAnalysisDF.to_csv('RecordAnalysis.csv',index_label='S.No.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#After Analyzing this CSV File (Copy of the table is in the report), we found that \"One Hot Encoded State,Removed Others - NAN removed with column with Linear Regression\" had the least R2Mean and RMSE Median. So, it had performed best in the 2/4 metrics we considered. Hence, we decided to go ahead with it."
   ]
  },
  {
   "source": [
    "### Testing"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#One Hot Encoded State,Removed Others\tLinear Regression\n",
    "FinalTrainDF = TrainDFX.drop(columns=['RegionName', 'Metro', 'CountyName'])\n",
    "#RemoveNulls(FinalTrainDF,True)\n",
    "AddStateSparseMatrix(FinalTrainDF)\n",
    "FinalTestDF = TestDFX.drop(columns=['RegionName', 'Metro', 'CountyName'])\n",
    "#FinalTrainDF[ValDF.name] = ValDF\n",
    "#RemoveNulls(FinalTrainDF,False)\n",
    "AddStateSparseMatrix(FinalTestDF)\n",
    "#FinalTrainDF.drop(columns=['State'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegression()\n",
    "hist = model.fit(FinalTrainDF,TrainDFY)\n",
    "YPred = model.predict(FinalTestDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "R2Score = r2score(TestDFY,YPred)\n",
    "RMSE = mse(TestDFY,YPred,squared=False)\n",
    "print('R2 score ', R2Score)\n",
    "print('Root Mean Square Error ', RMSE)\n",
    "print('Average of Output ',TestDFY.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lb.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for bed in range(1,5):\n",
    "    filterArr = (FinalTestDF['Bedrooms'] == bed)\n",
    "    #print(filterArr)\n",
    "    print('For '+ str(bed) +' bedrooms')\n",
    "    print('-'*5)\n",
    "    R2Score = r2score(TestDFY[filterArr],YPred[filterArr])\n",
    "    RMSE = mse(TestDFY[filterArr],YPred[filterArr],squared=False)\n",
    "    print('R2 score ', R2Score)\n",
    "    print('Root Mean Square Error ', RMSE)\n",
    "    print('Average of '+ str(bed) +' Bedroom Cost ',TestDFY[filterArr].mean())\n",
    "    print('-'*10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}